{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](http://res.cloudinary.com/dgzgmhbah/image/upload/v1535989910/modemly-forum-posts/iris-machinelearning_otj9an.png)"},{"metadata":{},"cell_type":"markdown","source":"**Hi Kagglers,\n**\n\n**Welcome to My Second Project. If there are any feedbacks/suggestions you would like to see in the Kernel please let me know. This notebook will always be a work in progress. Please leave any comments about further improvements to the notebook. I appreciate every note!\n**\n\n### If you like it , you can upvote and/or leave a comment :)\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Import Libraries and the Database **"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nfrom math import pi\n\n#Data Visualization libraries \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n# Import necessary modules\nfrom scipy.stats import randint\n\n#Selection the Model \nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import KMeans\n\n#Selection the Regulation \nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\n#Model Selection \nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n\n#Pipeline\nfrom sklearn.pipeline import Pipeline,make_pipeline\n\n#Data Preprocessing \nfrom sklearn.preprocessing import StandardScaler,scale,Imputer\n\n#Metrics “ Measure Model Performance” \nfrom sklearn.metrics import mean_squared_error,accuracy_score\nfrom sklearn.metrics import classification_report , confusion_matrix\nfrom sklearn.metrics import roc_curve ,roc_auc_score\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nprint(os.listdir(\"../input\"))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"iris = pd.read_csv(\"../input/Iris.csv\") #load the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Explore the Database **"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"iris.info()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"iris.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"iris.describe() #to know statistical data and the features scale ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visual Exploration for the Database **"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.subplot(2,2,1)\nsns.swarmplot(x='Species',y='PetalLengthCm',data=iris)\nplt.subplot(2,2,2)\nsns.swarmplot(x='Species',y='PetalWidthCm',data=iris)\nplt.subplot(2,2,3)\nsns.swarmplot(x='Species',y='SepalLengthCm',data=iris)\nplt.subplot(2,2,4)\nsns.swarmplot(x='Species',y='SepalWidthCm',data=iris)\nsns.set(style=\"whitegrid\", palette=\"muted\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The diagonal elements in a pairplot show the histogram by default,but we update it to \"kde\"\n# kde: which creates and visualizes a kernel density estimate of the underlying feature\niris = iris.drop(\"Id\", axis=1)\nsns.pairplot(iris, hue=\"Species\", size=3, diag_kind=\"kde\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notes that: There is correlation between the correlation between Petal Width and Length was very high.on the other hand, the Sepal Width and Length was very low.We can aprove that with heatmap.\n**\n\n**** This is very important notes because if there are features are highly correlated, the training data gives a much better accuracy.\n**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8)) \nsns.heatmap(iris.corr(),annot=True,cmap='Greens',cbar=False) #The correlation matrix\nplt.rc('xtick', labelsize=15)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=15)    # fontsize of the tick labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's play with different algorithm"},{"metadata":{},"cell_type":"markdown","source":"# Supervised Learning"},{"metadata":{},"cell_type":"markdown","source":"**We need to clear note:**\n**X = The attributes are the petal and sepal length and width. It is also known as Features**\n**y = Target variables Species**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=iris.loc[:,['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] #attributes\ny = iris.loc[:,'Species'] #Target Variables\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-Nearest Neighbours (Hyperparameter Tuning With Grid SearchCV )**"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_neighbors': np.arange(1, 50)}\nknn = KNeighborsClassifier()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\nknn_cv = GridSearchCV(knn, param_grid, cv=5) #Hyperparameter tuning using GridSearchCV \nknn_cv.fit(X_train, y_train)\ny_pred = knn_cv.predict(X_test)\nprint(knn_cv.best_params_)\nprint(\"KNN_CV Score:\",knn_cv.score(X_test, y_test))\n#or you can use Accuracy score \nprint('The accuracy of the K-Nearest Neighbours is',accuracy_score(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-Nearest Neighbours (Campare Accuracy between Sepal and Petal)**\n\n**** To Prove that : If there are features are highly correlated, the training data gives a much better accuracy.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN For Spal \nX_spal = iris.loc[:,['SepalLengthCm','SepalWidthCm']]\nparam_grid = {'n_neighbors': np.arange(1, 50)}\nknn = KNeighborsClassifier()\nX_train, X_test, y_train, y_test = train_test_split(X_spal, y, test_size=0.2, random_state=21)\nknn_cv_spal = GridSearchCV(knn, param_grid, cv=5) #Hyperparameter tuning using GridSearchCV \nknn_cv_spal.fit(X_train, y_train)\ny_pred = knn_cv_spal.predict(X_test)\nprint(\"Spal\")\nprint(knn_cv_spal.best_params_)\nprint(\"KNN_CV_spal Score:\",knn_cv_spal.score(X_test, y_test))\n#or you can use Accuracy score \nprint('The accuracy of the KNN For Spal is',accuracy_score(y_test, y_pred))\n\n\n#KNN For Patal \nX_patal = iris.loc[:,['PetalLengthCm','PetalWidthCm']]\nparam_grid = {'n_neighbors': np.arange(1, 50)}\nknn = KNeighborsClassifier()\nX_train, X_test, y_train, y_test = train_test_split(X_patal, y, test_size=0.2, random_state=21)\nknn_cv_patal = GridSearchCV(knn, param_grid, cv=5) #Hyperparameter tuning using GridSearchCV \nknn_cv_patal.fit(X_train, y_train)\ny_pred_patal= knn_cv_patal.predict(X_test)\nprint(\"____________________________\")\nprint(\"patal\")\nprint(knn_cv_patal.best_params_)\nprint(\"KNN_CV_patal Score:\",knn_cv_patal.score(X_test, y_test))\n#or you can use Accuracy score \nprint('The accuracy of the KNN for Patal is',accuracy_score(y_test, y_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression\n**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,mean_squared_error\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score\nlogreg=LogisticRegression()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nlogreg.fit(X_train,y_train)\ny_pred=logreg.predict(X_test)\n\n# Confusion matrix and Classification Report in scikit-learn\nprint('Confusuin_Martix:\\n',confusion_matrix(y_test, y_pred))\nprint('Classification Report: \\n ',classification_report(y_test,y_pred))\nprint('The accuracy of the Logistic Regression is',accuracy_score(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UnSupervised learning"},{"metadata":{},"cell_type":"markdown","source":"**Assume: The data given aren't labelled, which means only the input variables(X) are given with no corresponding output variables to make the algorithms discover interesting structures in the data without a specific prediction task in mind. **\n"},{"metadata":{},"cell_type":"markdown","source":"**KMeans Cluster**"},{"metadata":{"trusted":true},"cell_type":"code","source":"samples = iris.loc[:,['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] #attributes\n\n#First : Get the Best KMeans \nks = range(1,6)\ninertias=[]\nfor k in ks :\n    # Create a KMeans clusters\n    kc = KMeans(n_clusters=k)\n    kc.fit(samples)\n    inertias.append(kc.inertia_)\n\n# Plot ks vs inertias\nf, ax = plt.subplots(figsize=(15, 8))\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.style.use('ggplot')\nplt.title('What is the Best Number for KMeans ?')\nplt.show()\n\n#We choose an \"elbow\" in the inertia plot Where inertia begins to decrease more slowly\n#From the curve the best number of KMeans =  3 Clusters\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating a clustering by Cross Tabulation **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Second: The best number of KMeans =  3 Clusters\nkc = KMeans(n_clusters=3)\nkc.fit(samples)\nlabels = kc.predict(samples)\nprint(kc.inertia_)\n\n#Thrid: Evaluating a clustering by Cross Tabulation \ndf = pd.DataFrame({'labels': labels, 'species': iris.Species})\nct = pd.crosstab(df['labels'],df['species'])\n\nprint(ct)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}